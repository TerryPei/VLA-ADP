<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLA-ADP: Action-aware Dynamic Pruning for Efficient VLA Manipulation</title>

  <!-- Bulma CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700&family=Noto+Sans:wght@400;700&display=swap" rel="stylesheet">

  <style>
    /* ─── Global ─── */
    body {
      font-family: 'Noto Sans', sans-serif;
      background: #ffffff;
      color: #1a1a1a;
    }

    /* ─── Hero ─── */
    .hero-body {
      padding: 3rem 1.5rem 2rem;
    }
    .publication-title {
      font-family: 'Google Sans', sans-serif;
      font-size: 2.2rem;
      font-weight: 700;
      line-height: 1.25;
      color: #1a1a1a;
    }
    .venue-badge {
      display: inline-block;
      background: #2962ff;
      color: white;
      font-size: 0.95rem;
      font-weight: 600;
      padding: 0.25em 0.85em;
      border-radius: 999px;
      margin-bottom: 1rem;
      letter-spacing: 0.03em;
    }
    .publication-authors {
      font-size: 1.05rem;
      margin-top: 0.6rem;
    }
    .publication-authors a {
      color: #2962ff;
    }
    .author-block {
      display: inline-block;
      margin: 0 0.4em;
    }
    .affiliation {
      font-size: 0.92rem;
      color: #555;
      margin-top: 0.3rem;
    }

    /* ─── Link buttons ─── */
    .link-buttons {
      display: flex;
      flex-wrap: wrap;
      gap: 0.6rem;
      justify-content: center;
      margin-top: 1.2rem;
    }
    .link-btn {
      display: inline-flex;
      align-items: center;
      gap: 0.45em;
      padding: 0.5em 1.1em;
      border-radius: 999px;
      border: 2px solid #1a1a1a;
      font-size: 0.95rem;
      font-weight: 600;
      color: #1a1a1a;
      background: #fff;
      text-decoration: none;
      transition: background 0.15s, color 0.15s;
    }
    .link-btn:hover {
      background: #1a1a1a;
      color: #fff;
    }
    .link-btn .icon { font-size: 1em; }

    /* ─── Sections ─── */
    section.section {
      padding: 3rem 1.5rem;
    }
    .section-title {
      font-family: 'Google Sans', sans-serif;
      font-size: 1.75rem;
      font-weight: 700;
      margin-bottom: 1.2rem;
      color: #1a1a1a;
    }
    .section-divider {
      border: none;
      border-top: 1px solid #e0e0e0;
      margin: 0 0 2.5rem 0;
    }
    .abstract-text {
      font-size: 1.05rem;
      line-height: 1.75;
      color: #333;
      max-width: 820px;
      margin: 0 auto;
    }

    /* ─── Figures ─── */
    .figure-container {
      text-align: center;
      margin: 1.5rem auto;
      max-width: 1000px;
    }
    .figure-container img {
      max-width: 100%;
      border-radius: 6px;
      box-shadow: 0 2px 12px rgba(0,0,0,0.08);
    }
    .figure-caption {
      margin-top: 0.8rem;
      font-size: 0.92rem;
      color: #555;
      line-height: 1.6;
      max-width: 820px;
      margin-left: auto;
      margin-right: auto;
    }

    /* ─── Side-by-side figures ─── */
    .figure-row {
      display: flex;
      gap: 1.5rem;
      justify-content: center;
      align-items: flex-start;
      flex-wrap: wrap;
      max-width: 1000px;
      margin: 0 auto;
    }
    .figure-row .fig-item {
      flex: 1;
      min-width: 280px;
      text-align: center;
    }
    .figure-row .fig-item img {
      max-width: 100%;
      border-radius: 6px;
      box-shadow: 0 2px 12px rgba(0,0,0,0.08);
    }

    /* ─── Video grid ─── */
    .video-grid {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 1.2rem;
      max-width: 960px;
      margin: 0 auto;
    }
    @media (max-width: 600px) {
      .video-grid { grid-template-columns: 1fr; }
    }
    .video-card {
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 2px 12px rgba(0,0,0,0.08);
      background: #f8f8f8;
    }
    .video-card video {
      width: 100%;
      display: block;
    }
    .video-label {
      padding: 0.55rem 0.75rem;
      font-size: 0.85rem;
      color: #444;
      line-height: 1.45;
      border-top: 1px solid #e8e8e8;
    }

    /* ─── Results highlight boxes ─── */
    .result-highlights {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      justify-content: center;
      margin: 1.5rem 0;
    }
    .result-box {
      background: #f4f7ff;
      border-left: 4px solid #2962ff;
      border-radius: 6px;
      padding: 1rem 1.4rem;
      min-width: 200px;
      flex: 1;
      max-width: 280px;
    }
    .result-box .metric {
      font-size: 1.6rem;
      font-weight: 700;
      color: #2962ff;
      font-family: 'Google Sans', sans-serif;
    }
    .result-box .metric-label {
      font-size: 0.88rem;
      color: #555;
      margin-top: 0.2rem;
      line-height: 1.4;
    }

    /* ─── BibTeX ─── */
    .bibtex-block {
      background: #f4f4f4;
      border-radius: 8px;
      padding: 1.4rem 1.6rem;
      position: relative;
      font-family: 'Courier New', monospace;
      font-size: 0.88rem;
      line-height: 1.7;
      white-space: pre-wrap;
      max-width: 720px;
      margin: 0 auto;
      color: #2a2a2a;
    }
    .copy-btn {
      position: absolute;
      top: 0.8rem;
      right: 0.8rem;
      background: #2962ff;
      color: white;
      border: none;
      border-radius: 5px;
      padding: 0.3em 0.8em;
      font-size: 0.82rem;
      cursor: pointer;
      font-family: 'Noto Sans', sans-serif;
      transition: background 0.15s;
    }
    .copy-btn:hover { background: #1a4dcc; }
    .copy-btn.copied { background: #2e7d32; }

    /* ─── Footer ─── */
    footer.footer {
      background: #fafafa;
      border-top: 1px solid #ececec;
      padding: 2rem 1.5rem;
      text-align: center;
      font-size: 0.88rem;
      color: #777;
    }
    footer a { color: #2962ff; }
  </style>
</head>

<body>

<!-- ════════════════════════ HERO ════════════════════════ -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">

      <span class="venue-badge">ICLR 2026</span>

      <h1 class="publication-title">
        Action-aware Dynamic Pruning for<br>
        Efficient Vision-Language-Action Manipulation
      </h1>

      <div class="publication-authors" style="margin-top:1rem;">
        <span class="author-block"><a href="#">Xiaohuan Pei</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Yuxing Chen</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Siyu Xu</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Yunke Wang</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Yuheng Shi</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Chang Xu</a><sup>1</sup></span>
      </div>

      <div class="affiliation">
        <sup>1</sup>University of Sydney
      </div>

      <div class="link-buttons">
        <a href="https://arxiv.org/abs/2509.22093" class="link-btn" target="_blank">
          <span class="icon"><i class="fas fa-file-pdf"></i></span>
          <span>Paper</span>
        </a>
        <a href="https://github.com/TerryPei/VLA-ADP" class="link-btn" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span>
          <span>Code</span>
        </a>
        <a href="https://arxiv.org/abs/2509.22093" class="link-btn" target="_blank">
          <span class="icon"><i class="ai ai-arxiv"></i></span>
          <span>arXiv</span>
        </a>
      </div>

    </div>
  </div>
</section>

<!-- ════════════════════════ TEASER / MOTIVATION ════════════════════════ -->
<section class="section" style="padding-top:1rem;">
  <div class="container is-max-desktop">
    <div class="figure-container">
      <img src="assests/motivation.png" alt="Motivation: token redundancy varies across manipulation stages">
      <p class="figure-caption">
        <b>Motivation.</b> Visual token redundancy varies significantly across robot manipulation stages.
        VLA-ADP exploits end-effector motion as a dynamic gating signal to identify and prune
        redundant tokens at each timestep, reducing computation without sacrificing task success.
      </p>
    </div>

    <!-- Key metrics directly below teaser -->
    <div class="result-highlights" style="margin-top:2rem;">
      <div class="result-box">
        <div class="metric">1.49×</div>
        <div class="metric-label">Real-world latency speedup<br>(76.9 ms → 51.8 ms)</div>
      </div>
      <div class="result-box">
        <div class="metric">88.3%</div>
        <div class="metric-label">Real-world success rate<br>(up from 85.8% baseline)</div>
      </div>
      <div class="result-box">
        <div class="metric">1.35×</div>
        <div class="metric-label">LLM speedup on LIBERO<br>at 30–40% token keep ratio</div>
      </div>
      <div class="result-box">
        <div class="metric">≤0.9%</div>
        <div class="metric-label">SR drop at 50–70%<br>keep ratio (LIBERO)</div>
      </div>
    </div>

  </div>
</section>

<!-- ════════════════════════ ABSTRACT ════════════════════════ -->
<section class="section" style="background:#fafafa; padding: 2.5rem 1.5rem;">
  <div class="container is-max-desktop">
    <h2 class="section-title has-text-centered">Abstract</h2>
    <hr class="section-divider">
    <p class="abstract-text has-text-centered">
      We propose <b>Action-aware Dynamic Pruning (ADP)</b>, a training-free, plug-and-play method
      that adaptively prunes redundant visual tokens across manipulation stages by combining
      <em>text-driven token relevance</em> with an <em>action-aware gating signal</em> derived from
      end-effector motion.
    </p>
  </div>
</section>

<!-- ════════════════════════ METHOD ════════════════════════ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="section-title has-text-centered">Method Overview</h2>
    <hr class="section-divider">

    <div class="figure-row">
      <div class="fig-item" style="flex: 2;">
        <img src="assests/main2.png" alt="ADP method overview">
        <p class="figure-caption" style="text-align:left;">
          <b>ADP Architecture.</b> ADP maintains an observation window of past states and uses
          end-effector velocity/acceleration to produce a dynamic gating decision. The gate selects
          between <em>sparse</em> and <em>dense</em> token retention ratios, and text-driven
          cross-attention scores rank tokens by relevance before pruning.
        </p>
      </div>
      <div class="fig-item" style="flex: 1;">
        <img src="assests/prune3.png" alt="Token pruning visualization">
        <p class="figure-caption" style="text-align:left;">
          <b>Token Pruning.</b> Spatially redundant background tokens (low attention score) are
          removed while task-relevant tokens are preserved, maintaining action prediction fidelity.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ════════════════════════ DEMO VIDEOS ════════════════════════ -->
<section class="section" style="background:#fafafa;">
  <div class="container is-max-desktop">
    <h2 class="section-title has-text-centered">Real-World Demonstrations</h2>
    <hr class="section-divider">
    <p class="has-text-centered" style="margin-bottom:1.5rem; color:#555; font-size:0.97rem;">
      VLA-ADP applied to OpenVLA-OFT on 4 real-world ALOHA manipulation tasks.
      All episodes shown are successful rollouts.
    </p>

    <div class="video-grid">

      <div class="video-card">
        <video controls muted playsinline loop preload="metadata">
          <source src="assests/2025_09_11-19_42_44--openvla_oft--episode=7--success=True--task=put_the_white_mug_on_the_plate_and_put_the_chocola.mp4" type="video/mp4">
        </video>
        <div class="video-label">
          <b>Task 1:</b> Put the white mug on the plate and put the chocolate on the plate
        </div>
      </div>

      <div class="video-card">
        <video controls muted playsinline loop preload="metadata">
          <source src="assests/2025_09_11-19_47_37--openvla_oft--episode=4--success=True--task=open_the_top_drawer_and_put_the_bowl_inside.mp4" type="video/mp4">
        </video>
        <div class="video-label">
          <b>Task 2:</b> Open the top drawer and put the bowl inside
        </div>
      </div>

      <div class="video-card">
        <video controls muted playsinline loop preload="metadata">
          <source src="assests/2025_09_11-19_57_31--openvla_oft--episode=6--success=True--task=pick_up_the_tomato_sauce_and_place_it_in_the_baske.mp4" type="video/mp4">
        </video>
        <div class="video-label">
          <b>Task 3:</b> Pick up the tomato sauce and place it in the basket
        </div>
      </div>

      <div class="video-card">
        <video controls muted playsinline loop preload="metadata">
          <source src="assests/2025_09_11-20_01_43--openvla_oft--episode=9--success=True--task=pick_up_the_black_bowl_next_to_the_plate_and_place.mp4" type="video/mp4">
        </video>
        <div class="video-label">
          <b>Task 4:</b> Pick up the black bowl next to the plate and place it on the rack
        </div>
      </div>

    </div>
  </div>
</section>

<!-- ════════════════════════ RESULTS ════════════════════════ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="section-title has-text-centered">Experimental Results</h2>
    <hr class="section-divider">

    <!-- LIBERO table -->
    <h3 class="has-text-centered" style="font-size:1.2rem; font-weight:600; margin: 2rem 0 0.8rem; color:#333;">
      Simulation Results (LIBERO Benchmark)
    </h3>
    <div class="figure-container">
      <img src="assests/libero_table.png" alt="LIBERO simulation results table" style="max-width:900px;">
      <p class="figure-caption">
        Comparison against OpenVLA, SparseVLM, FastVLM, and other VLA methods across four LIBERO
        task suites (Spatial, Object, Goal, Long). VLA-ADP achieves 94.4–99.0% success rate with
        1.13–1.35× LLM speedup.
      </p>
    </div>

    <!-- LIBERO visualization -->
    <div class="figure-container" style="margin-top:1.5rem;">
      <img src="assests/libero.png" alt="LIBERO task suite visualization">
      <p class="figure-caption">
        LIBERO benchmark task suites used for simulation evaluation: Spatial, Object, Goal, and Long.
      </p>
    </div>

    <!-- Real-world table -->
    <h3 class="has-text-centered" style="font-size:1.2rem; font-weight:600; margin: 2.5rem 0 0.8rem; color:#333;">
      Real-World Results (ALOHA Robot)
    </h3>
    <div class="figure-container">
      <img src="assests/real_table.png" alt="Real-world results table" style="max-width:700px;">
      <p class="figure-caption">
        VLA-ADP improves success rate from 85.8% to 88.3% while reducing inference latency by
        33% (76.9 ms → 51.8 ms), achieving a <b>1.49× speedup</b> on real hardware.
      </p>
    </div>

    <!-- Real robot photo -->
    <div class="figure-container" style="margin-top:1.5rem;">
      <img src="assests/real.jpg" alt="Real-world robot setup" style="max-width:700px;">
      <p class="figure-caption">
        Real-world experimental setup: bimanual ALOHA robot performing tabletop manipulation tasks.
      </p>
    </div>

  </div>
</section>

<!-- ════════════════════════ CITATION ════════════════════════ -->
<section class="section" style="background:#fafafa;">
  <div class="container is-max-desktop">
    <h2 class="section-title has-text-centered">Citation</h2>
    <hr class="section-divider">
    <div style="position:relative; max-width:720px; margin:0 auto;">
      <pre class="bibtex-block" id="bibtex-text">@article{pei2025action,
  title={Action-aware dynamic pruning for efficient
         vision-language-action manipulation},
  author={Pei, Xiaohuan and Chen, Yuxing and Xu, Siyu
          and Wang, Yunke and Shi, Yuheng and Xu, Chang},
  journal={arXiv preprint arXiv:2509.22093},
  year={2025}
}</pre>
      <button class="copy-btn" id="copy-btn" onclick="copyBibTeX()">Copy</button>
    </div>
  </div>
</section>

<!-- ════════════════════════ ACKNOWLEDGEMENTS ════════════════════════ -->
<section class="section" style="padding-bottom: 2rem;">
  <div class="container is-max-desktop">
    <h2 class="section-title has-text-centered">Acknowledgements</h2>
    <hr class="section-divider">
    <p class="abstract-text has-text-centered" style="font-size:0.97rem;">
      We thank the authors of
      <a href="https://github.com/moojink/openvla-oft" target="_blank">OpenVLA-OFT</a>,
      <a href="https://github.com/openvla/openvla" target="_blank">OpenVLA</a>, and
      <a href="https://huggingface.co/docs/transformers" target="_blank">Hugging Face Transformers</a>
      for making their code publicly available.
      This project page was inspired by the
      <a href="https://nerfies.github.io/" target="_blank">Nerfies</a> template.
    </p>
  </div>
</section>

<!-- ════════════════════════ FOOTER ════════════════════════ -->
<footer class="footer">
  <p>
    &copy; 2025 VLA-ADP Authors. Released under the
    <a href="https://github.com/TerryPei/VLA-ADP/blob/main/LICENSE" target="_blank">Apache 2.0 License</a>.
  </p>
  <p style="margin-top:0.4rem;">
    Page template adapted from
    <a href="https://nerfies.github.io/" target="_blank">Nerfies</a>.
  </p>
</footer>

<script>
  function copyBibTeX() {
    const text = `@article{pei2025action,
  title={Action-aware dynamic pruning for efficient vision-language-action manipulation},
  author={Pei, Xiaohuan and Chen, Yuxing and Xu, Siyu and Wang, Yunke and Shi, Yuheng and Xu, Chang},
  journal={arXiv preprint arXiv:2509.22093},
  year={2025}
}`;
    navigator.clipboard.writeText(text).then(() => {
      const btn = document.getElementById('copy-btn');
      btn.textContent = 'Copied!';
      btn.classList.add('copied');
      setTimeout(() => {
        btn.textContent = 'Copy';
        btn.classList.remove('copied');
      }, 2000);
    });
  }
</script>

</body>
</html>
